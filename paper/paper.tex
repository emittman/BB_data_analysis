\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf,float}
\graphicspath{C:/Users/Colin/Documents/GitHub/BB_data_analysis/paper/fig/}
\usepackage{enumerate}
\usepackage[numbers]{natbib}
\usepackage{url} % not crucial - just used below for the URL

% math macros 
\newcommand{\ind}{\stackrel{ind.}{\sim}}
\newcommand{\op}{\operatorname}

\newcommand{\myequation}{\begin{equation}}
\newcommand{\myendequation}{\end{equation}}
\let\[\myequation
\let\]\myendequation


\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf A Hierarchical Bayesian Approach for Modeling Infant-Mortality and Wearout Failure Modes}
  \author{Eric Mittman 1\thanks{
    The authors gratefully acknowledge Bill Meeker for his comments and suggestions}\hspace{.2cm}\\
    Department of Statistics, Iowa State University\\
    and \\
    Colin Lewis-Beck \\
    Department of Statistics, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  100 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, (don't reuse words appearing in title)
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
Failure distributions of high-reliability products, can be difficult
to assess. Accelerated testing tries to address this problem, but
requires strong assumptions to extrapolate to use conditions in the
field (citation). When field data are available, information may be
very limited. In this paper, we propose the use of Bayesian hierarchical modeling to borrow
information to improve inferences on lifetime
distributions where information is limited by right-censoring and
left-truncation.

Failure data for series systems may be collected at the system level
by end-users interested only in the lifetime of the system. \citet{chan} proposed the generalized limited failure population
model (GLFP), which provides a generative model for
failure times in a population where some units are susceptible to infant mortality. This model
covers a subset of the class of distributions known as bathtub distributions
\cite{sujata}. Bathtub distributions are characterized by a U-shaped
hazard function, where early failures are due to infant mortality and
late failure are due to wearout. While complex systems actually
exhibit more than two modes of failure, suitable parametric models
which can suitably approximate the overall lifetime distribution are
useful. 

In the case of complex, high-reliability systems where cause of failure is
not available, a model which
\begin{enumerate}[a.]
\item has parameters that can be estimated from the observed data
\item is flexible enough to fit the observed data
\item in interpretable with respect to available theory of the underlying process
\end{enumerate}
is desired. We present a Bayesian, hierarchical modeling approach for
grouped failure data. As a benefit of working with Monte Carlo
samples, we can easily make inference on a wide range of quantities of
interest while accounting for uncertainty. We demonstrate this approach on real data hard
drive failure data from a
cloud-based storage company.

\subsection{Background}
In engineering applications, a product can often fail from one out of a set of possible malfunctioning components.  For example, a computer system can fail if the mother board, disc drive or power supply stop working.  Circuit boards (CB) can fail due to a manufacturing defect or later as a result of wearout.  The general name for such products is a series system where the lifetime of the product is the minimum failure time across $s$ different components or risks \cite{nelson}.  A common assumption in series systems is the time to failure for each risk, $s$, is statistically independent.  Thus, the overall reliability of a unit is modeled using the product rule across all $s$ risks.  Parameter estimation is straightforward if the cause of failure is known for each observation.  With engineering systems data, however, the exact cause of failure is frequently unknown or masked from the researcher.  \\

Previous papers have employed various data assumptions and methodologies to model masked lifetime failure data.  When modeling computer system failures Reiser et al. assumed each observed failure came from a known subset of failure modes, and estimation was performed using a Bayesian approach \cite{reiser}.  Chan and Meeker labeled the cause of circuit board failures as infant mortality, unknown, or wearout based on the time of observed failures.  This helped identify parameters when using maximum likelihood (ML) estimation.  Extending Chan and Meeker's analysis, Basu et. al performed a Bayesian analysis with informative priors to better identify early versus late failure modes without making any data assumptions \cite{basu}.  Berger and Sun introduced the Poly-Weibull distribution where cause of failure is the minimum of a several Weibull distributions \cite{berger}.  More recently, Ranjan et al. considered a competing risk model for infant mortality and wearout as a mixture of Weibull and exponential failure distributions \cite{ranjan}.  Treating the unknown failure modes as incomplete data, an expectation maximization algorithm with ML was used, in addition to Bayesian estimation.

\subsection{Motivation}
The goal of this paper is to model, and compare, the lifetime distribution of different hard drive brands using the GLFP lifetime model.  Product populations often contain a mixture of defective and non defective units.  The hazard function for this type of population is often described as a bathtub curve: the beginning of the curve corresponds to defective units failing early, followed by a constant hazard, and then and upswing as units fail from wearout.  Ignoring this hazard structure, which exists for series systems, could lead to spurious inference when comparing the reliability of hard drive brand models \cite{sujata}.  The GLFP combines different failure time distributions to account for bending hazard functions; for example, one parametric model for defective units (infant mortality) and a second model for wearout \cite{chan}. \\ 

While the GLFP model allows for multiple hazard functions, it does not account for the group structure of the hard drive data.  A naive approach would combine all drive models and fit an overall GLFP model.  This model can be easily estimated, but a common GLFP model for all drive brand groups fits poorly: the model captures drive models with lots of observed failures, while the predicted time to failure for drive brand models with less information is over or under estimated.  At the other extreme, fitting individual models to each hard drive brand excludes many drive brand models with fewer than 5 failures.  The GLFP model applied to these data produces very imprecise parameter estimates or can only be fit if highly informative priors are imposed.  \\

We therefore propose to fit the GLFP model using a hierarchical Bayesian approach.  With over 60 brand drive models in testing a hierarchical model is advantageous as it pools information from across brands to get more precise estimates of the GLFP parameters for each drive model group.  Moreover, the hierarchical model is a nice compromise between the aggregate and individual modeling approaches: drive models with lots of failures are estimated precisely; and in cases where little data is available the hierarchical model borrows information from other drive brand models to obtain stable estimates.  This allows us to include all of the hard drive data and more importantly, make valid inferences for each group of hard drive brands.

\subsection{Overview}
The structure of the paper is as follows.  Section 1 summarizes the Backblaze data and the unique modeling challenges it presents.  Section 2 introduces the GLFP model as a mixture of two Weibull distributions.  In Section 3 we fit multiple specifications of the GLFP model to best describe the Backblaze data.  We also discuss computational and modeling issues that suggest a final model with a common Weibull distribution for defective units and individual Weibull distributions for the wearout failure mode.  In Section 4 we compare hard drive brand models based on our final GLFP model.  Finally, in Section 5, we review the GLFP model and propose future extensions.

\section{Data}
Backblaze is a company that offers backup storage to protect against data loss.  While selling storage space is Backblaze's main business, since 2013 it has been collecting data on hard drives operating at its facility.  The purpose is to provide consumers and businesses with reliability information on different hard drive brands and models.  The hard drives continuously spin in controlled storage pods where they run until failure.  When a hard drive fails it is permanently removed, and new hard drives are regularly added to the storage pods.  In addition, the number of storage pods is increasing as Backblaze adds new hard drive brands to the sample.  Every quarter Backblaze makes its data publicly available through their website \cite{backblaze}. In addition, Backblaze publishes summary statistics of the different hard drive models currently operating.  No other analysis or modeling of the failure data is provided; however, Backblaze does encourage the public to further analyze its data, which for this paper goes through the first quarter of 2016.

As of the first quarter of 2016, Backblaze was collecting data on 63 different hard drive models.  Some drive models have been running since 2013, while others have been added at a later date.  There are a total of 75,297 hard drives running.  The distribution of drives by model varies: some models have only have a service record for a single drive; the maximum number of service records for a single drive model is 35,860.  We can also look at the distribution of total number of failures and total time on test across all drive models.  As seen below, some drive models contain a lot of information, while others have few failures or a little time in operation. [INSERT FIGURE 1]. \\

Probability plotting is an effective method to check the adequacy of various parametric models to the data.  Identifying whether failure data follows a specific distribution is difficult to do by eye: especially when looking at pdfs or other non-linear plots.  However, if we can linearize the cumulative distribution function it is easier to visually assess a distributional goodness of fit; if a model is appropriate, a non parametric estimate of $\hat{F(t)}$ graphed on linarized probability scales should approximately follow a straight line. \\

For the hard drive data we first estimate the empirical cdf for each model using the Kaplan-Meier estimator \cite{kaplan}.  With left truncation, however, the Kaplan-Meier estimate, $\widehat{F(t)_{KM}}$, is conditional based on hard drive $i$ surviving up to time $\tau_i^L$ where $L$ is the amount of time the hard drive was running before Backblaze starting monitoring the drive.  To get the unconditional distribution we correct the non-parametric estimates using a parametric adjustment outlined by Turnbull, and given in more detail by Meeker and Escobar \cite{turnbull,meeker}.  For each hard drive model we select $\tau_{\text{min}}^L$, the smallest left truncated time in the sample.  Then, using the parametric estimates of the parameters, we calculate $Pr(T>\tau_\text{min}^L)$ the probability a hard drive has survived up to $\tau_{\text{min}}^L$.  We then apply this correction to the non parametric estimates, which results in the unconditional distribution of time to failure.\\

\begin{figure}
\includegraphics[width=.8\textwidth]{fig/data_overview}
\caption{Left: Kaplan-Meier plots showing nonparametric estimates of fraction failing for individual drive-models with truncation adjustments based on fitted model, Right: Total observed failures (top) and the sum of lengths of all observational intervals by drive-model.}
\end{figure}

In Figure \ref{fig2} we plot the Kaplan-Meier adjusted cdf for hard drive model X on Weibull paper.  Each point on the plot corresponds to a hard drive failure.  Censored drives are not plotted.  As mentioned in the introduction, the population of hard drives exhibits two primary failure modes.  One mode is a result of manufacturing defects, which cause early failures, known as infant mortality.  The second mode is non-defective hard drives that eventually fail due to wearout.   Evidence of at least two failure modes is seen in the Kaplan Meier plot with a kink occurring around hour Z.  Therefore, fitting a single Weibull model would not be flexible enough to model the failure distribution.


\section{Hierarchical GLFP model}
For modeling the lifetime of hard-drives, we select the Generalized Limited Failure Population model of \citet{chan}.
Let $T_{d,i}$ be the time of failure for the $i^{th}$ drive of drive-model $d$.
We assume that $T_{d,1},\ldots T_{d,n_d}$ are independent and have a probability distribution with cdf given by
$$P(T_{d,i}\le t) = 1 - (1-\pi_d\, F_{d1}(t))(1 - F_{d2}(t)), \mbox{ for }t>0 \mbox{, and where } \pi_d \mbox{ is in }(0,1).$$

As in \cite{chan}, we assume $F_{dj}$ is a member of the Weibull family of cdfs and parameterize in terms of a log-location parameter $\mu_{dj}$ and log-scale parameter $\sigma_{dj}$ so that

$$F_{dj}(t) = 1 - \exp \left\{ -\exp \left\{ \frac{ \log (t) - \mu_{dj}}{\sigma_{dj}} \right\} \right\},\; t>0, j=1,2$$

For the purpose of exposition, we will refer to the probability distributions $F_{d1}$ and $F_{d2}$ as ``failure modes." Specifically, we will refer to $F_{d1}$ as the ``early failure mode", and $F_{d2}$ as the ``main failure mode." The interpretation of the parameter $\pi_d$ is to represent the proportion of units susceptible to early failure, hence susceptible to both failure modes. Here the cause of failure is not assumed to be known, thus units of the same drive-model are exchangable.

To borrow strength across models, we can either share parameters across drive-models, or model the drive-model specific parameters hierarchically, allowing the data to inform the hyperparameters. For the second option, we model the scales, $\sigma_j$, quantiles, $t_{p_j,d,j} = \Phi^{-1}(\mu_j)$, and proportions, $\pi_d$, of the component distributions as follows:

$$\sigma_{d,1} \ind \op{Lognormal} \left( \eta_{\sigma,1}, \tau^2_{\sigma,1} \right) \mbox{ for } d=1,\ldots,D$$
$$\sigma_{d,2} \ind \op{Truncated-Lognormal} \left( \eta_{\sigma,2}, \tau^2_{\sigma,2} , 0, 1, \right) \mbox{ for } d=1,\ldots,D$$
$$t_{p_j,d,j} \equiv \mu_{d,j} + \sigma_{d,j}\,\Phi^{-1}(p_j)  \ind \op{Normal} \left(\eta_{t_{p_j},j}, \tau^2_{t_{p_j},j}\right) \mbox{ for } j=1,2\; d=1,\ldots,D$$

$$\op{logit} \pi_d \ind \op{N}(\eta_pi, \tau_pi) \mbox{ for } d=1,\ldots,D.$$

Here, $\Phi^{-1}$ is the quantile function of the standard log-Weibull distribution. We truncate $\sigma_{d,2}$ at $1$, appealing to the logic that,by nature, wearout produces an increasing hazard function. The decision to parameterize in terms of a quantile other than the log-location parameter, $\mu = t_{0.632}$, is that lifetime data often, as is true of the data presented here, features heavy right-censoring where inferences about the location parameter are extrapolations beyond the range of the data. For this data we selected $p_1=0.5,\mbox{ (the median), and } p_2 = 0.2$.

We consider the models with the following set of restrictions (from most to least restrictive):

\begin{enumerate}
\item $\pi_{d} = \pi,\quad \mu_{d1} = \mu_1,\quad \sigma_{1}=\sigma_1,\quad \mu_{d2} = \mu_2,\quad \sigma_{d2} = \sigma_2$
\item $\pi_{d} = \pi,\quad \mu_{d1} = \mu_1,\quad \sigma_{d1}=\sigma_1,\quad \sigma_{d2} = \sigma_2$
\item $\pi_{d} = \pi,\quad \mu_{d1} = \mu_1,\quad \sigma_{d1}=\sigma_1$
\item $\mu_{d1} = \mu_1,\quad \sigma_{d1}=\sigma_1$
\end{enumerate}

The set of model specifications were chosen based on the data, interpretation of the model, as well as estimation considerations. Drive brand specific parameters for the the wearout failure mode $(\mu_{d2},\sigma_{d2})$, and the proportion defective $(\pi_d)$, are considered as a means to account for heterogeneity across brands in the right tails of the failure distribution.  Going from a common model for all hard drive brands and gradually increasing the complexity of the model, we end up at Model 4.  Model 4 allows for the probability of infant mortality as well as the shape and scale parameters for the 2nd failure mode to vary by brand drive model.

For all of the models we consider, the parameters for the infant mortality failure mode are held in common across drive brand models. We found that there was insufficient information in the data to model these parameters hierarchically.  Moreover, assuming a common distribution for infant mortality provides a meaningful interpretation and comparison of $\pi_i$ and $\pi_j$ ($i \neq j$). 

\subsection{Priors}
To complete the full probability model, we need to select prior distributions for the parameters governing the hierarchical model. We select proper priors to ensure a proper posterior.

For models 1-4, different sets of restrictions required different prior specifications, which were assigned as follows:

\begin{enumerate}
\item This model constrains all drive models to the same failure distribution. For this ``reduced" model we assume the priors :

$$\mbox{logit}^{-1}\pi_1 \sim \op{Normal}(-3,2),\quad \sigma_1 \sim \op{Lognormal}(0, 2.5), \quad t_{0.5,1} \sim \op{Normal}(8,4)$$
$$\sigma_2 \sim \op{Lognormal}(0, 2.5) \quad t_{0.2,2} \sim \op{N}(10,4)$$

The prior on $\pi$ implies with $90\%$ probability $\pi$ is in the interval $(.002, .572)$. The prior on $\sigma_1$ and $\sigma_2$ put it on the interval $(0.02, 61.1)$ with $90\%$ probability. (Note that, due to symmetry, the Weibull shape parameter has the same prior interval and that a value of 1, corresponding to a constant hazard, is the median of this prior distribuion.) $t_{0.5,1}$ has a prior $90\%$ interval of $(1.4,14.6)$. For $t_{0.2,2}$ it is $(3.4,16.6)$. Since these last two parameters are on the scale of log-hours, admitting values from hours to decades, we consider them to be relatively uninformative.

\item We allow $t_{pd}$ to vary by drive-model. To help with model identifiability, we tighten the priors on the defective mode:
$$ \mbox{logit}^{-1}\pi \sim \op{Normal}(-3,1),\quad \sigma_1 \sim \op{Lognormal}(0, 1), \quad t_{p1} \sim \op{Normal}(7,2),$$
implying now a prior $90\%$ probability that $\sigma_1$ is between $0.19$ and $5.18$ and $t_{0.5,1}$ between $3.7$ and $10.3$, which, translating from the log-scale, is equivalent to an interval from 1.5 days to 3.4 years.

\item $\sigma_{d2}$ is allowed to vary. Priors for constrained parameters are the same as for model 2.

\item $\pi_d$ varies. Priors for constrained parameters are the same as for model 2.

\end{enumerate}

For the scale hyper-parameters we follow the recommendations of \citet{gelman2014bayesian} and use half-Cauchy priors. As for the location hyper-parameters, we select weakly informative priors consistent with our prior information on hard-drives. The prior for $\eta_\pi$ puts 95\% of prior mass on the interval $(0.006, 0.27)$ for the median proportion defective (What justification? Should maybe be lower.) For $\eta_{\sigma, 2}$, $95\%$ of prior mass is on values greater than $0.037$. Since $\sigma_d$ is the reciprocal of the Weibull shape parameter, this correponds roughly to an assumption that the median Weibull shape parameter is less than $1/0.037 = 27$. The prior for $\eta_{t_{p_2},2}$ implies that the median 20th percentile for non-defective units is less than greater than 3 days and less than 24 years.

\begin{align*}
  \eta_{\pi} & \sim \op{Normal}(-3, 1)\\
  \tau_{\pi} & \sim \op{Cauchy}^+(0, 1)\\
  \eta_{\sigma ,2} & \sim \op{Normal}(0, 2)\\
  \tau_{\sigma ,2} & \sim \op{Cauchy}^+(0, 1)\\
  \eta_{t_{.2_2},2} & \sim \op{Normal}(9, 2)\\
  \tau_{t_{.2_2},2} & \sim \op{Cauchy}^+(0, 1)
 \end{align*} 

\subsection{Computation}
Each model was fit using the {\tt
  rstan}\cite{rstan} package in {\tt R} \cite{r}, which implements a variant of Hamiltonian Monte Carlo (HMC)
\cite{betancourt}. Multiple chain were run for 1500 iterations after 1500 warmup iterations. Convergence was assessed by examining posterior
plots and checking that potential scale reduction factors (Rhat) \cite{gelman2014bayesian} were less than $1.1$. Four chains were run for models 1,2 and 3 and 16 chains were run for model 4.

\section{Data analysis}


\subsection{Model Comparisons}
\begin{figure}
\includegraphics[width=\textwidth]{fig/heterogeneity_compare_preliminary.pdf}
\caption{Left: Kaplan-Meier estimates of the time to failure for each of the drive-models in the Backblaze data. Right: Pointwise posterior median time to failure curves for Models 1-4, orderered left to right, top to bottom.}
\label{fig2}
\end{figure}

The right side of Figure \ref{fig2} shows that the set of estimated failure curves for all four models that we fit. The upper left panel shows the estimated failure curve fit to all data ignoring drive model as a factor (Model 1). The upper left plot, which represents Model 2, demonstrates that there is appreciable differentiation between drive models; note that, since the scale parameter, $\tau_{t_{p2}}$ is learned hierarchically from the data, we can ascertain that there is evidence in the data that this variability is real.

The lower left panel, representing Model 3, suggests differing shape parameters in the wearout failure mode which mean that the ranking of models with respect to fraction failing differs as a function of time.

Finally, the lower right panel, corresponding to Model 4, shows that, by allowing $\pi_d$ to vary by drive model, there is greater variability in the early rate of failure and, for some drive models, the forecasted fraction failing is adjusted downward. 
First look at the overlaid posterior credible bands.  Perhaps pick 4 different drive models and present a plot like the figure below.  Bands are for full model, and the solid line is the full model fit. \\

\begin{figure}[H]
    \centering
   \includegraphics[width=5.0in]{fig/mod40_bands.pdf}
		\caption{Drive Model 40 \label{fig:first}} 
\end{figure}


In addition to graphically examining model fits via posterior credible bands, we compared the predictive accuracy of the 4 different models using an approximate leave-one-out cross validation (LOO) method.  As outlined in Vehtari et. al, the predictive accuracy from a fitted Bayesian model can be estimated simply with posterior draws of the model parameters \cite{vehtari}.  For each observation, $i$, the log point-wise predictive density is calculated over the full set of posterior samples with the $i$th data point removed.  The final expected point-wise predictive density is the sum over all observations (elpd $=\sum{\log p(y_i|y_{-i})}$).  We computed the elpd for all 4 models using the R package loo \cite{loo}.  When $n$ is large, the distribution of the elpd is approximately normal and models can be statistically compared.  We calculated the difference in expected predictive accuracy for Model 4 vs 3, Model 3 vs 2, and Model 2 vs 1, as well as the standard error of the difference (Table 1).  As the model complexity increased, the expected predictive accuracy improved.  Of all the models, Model 4 had the best predictive accuracy and was significantly better than the other 3 models.

\begin{table}[H]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & elpd\ & Diff(elpd)\ & SE(Diff) \\ 
  \hline
Model 4 & -13309.5 & 40.7 & 11.3 \\ 
Model 3 & -13350.2 & 458.8 & 31.0  \\ 
Model 2 & -13809.0 & 3674.6 & 96.2 \\ 
Model 1 & -17483.6  \\ 
   \hline
\end{tabular}
\caption{Model Comparison Based on LOO}
\label{table:1}
\end{table}


\subsection{Brand Comparisons}



\begin{figure}[H]
    \centering
   \includegraphics[width=5.0in]{fig/B10_full.pdf}
		\caption{B10 in Years for All Drive Models \label{fig:first}} 
\end{figure}

Am wondering if we should truncate this graph at a reasonable value, perhaps .3 or .2?  

\begin{figure}[H]
    \centering
   \includegraphics[width=5.0in]{fig/pi_compare.pdf}
		\caption{$\pi$ for All Drive Models \label{fig:first}} 
\end{figure}

\section{Discussion}
Review the advantages of fitting the GFLP model and offer future ideas.  

Advances in computer technology have made computationally intensive Bayesian methods for fitting hierarchical models feasible in practice. Analysis using HMC is accessible to practitioners by Stan. HMC can be very efficient in terms of effective samples because of the way that it uses joint updates for all parameters using Hamiltonian dynamics to rapidly explore the target distribution. Hierarchical modeling can be a useful way to pool information to estimate large models. By using Bayesian methods like the one presented here we can avoid using potentially problematic asymptotic approximations, since all posterior uncertainty is automatically integrated into averages computed from functions of the samples.



    


\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}

\item Put R Stan code here

\end{description}

\bibliographystyle{plainnat}
\bibliography{sample}

\end{document}
